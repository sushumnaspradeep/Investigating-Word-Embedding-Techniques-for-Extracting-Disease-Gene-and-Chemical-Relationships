{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "061ab2cb-84c3-43b6-aa5b-e7a6860f5192",
   "metadata": {},
   "source": [
    "# Disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500b6c2e-9e2e-41fc-a796-cd5e2b35d596",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "def process_files(json_file_path, csv_file_path, output_directory):\n",
    "    # Load JSON data\n",
    "    with open(json_file_path, 'r') as json_file:\n",
    "        json_data = json.load(json_file)\n",
    "\n",
    "    # Load disease annotations from CSV\n",
    "    annotations_df = pd.read_csv(csv_file_path)\n",
    "\n",
    "    # Extract unique words from the JSON data\n",
    "    all_words = set()\n",
    "    for entry in json_data:\n",
    "        text = entry.get(\"ARTICLE\", {}).get(\"TEXT\", \"\")\n",
    "        words = nltk.word_tokenize(text)\n",
    "        all_words.update(words)\n",
    "\n",
    "    # Tokenize each entry in the 'annotation_text' column, ensuring that each element is a string\n",
    "    annotations_df['annotation_words'] = annotations_df['annotation_text'].apply(\n",
    "        lambda x: nltk.word_tokenize(str(x)) if pd.notnull(x) else []\n",
    "    )\n",
    "\n",
    "    # Filter annotations based on unique words\n",
    "    filtered_annotations = annotations_df[annotations_df['annotation_words'].apply(lambda x: any(word in all_words for word in x))]\n",
    "\n",
    "    # Train Word2Vec model (SkipGram)\n",
    "    model = Word2Vec(sentences=[list(all_words)], vector_size=250, window=6, min_count=1, sg=1)\n",
    "\n",
    "    # Get unique phrases, their MN values, and embeddings using average pooling\n",
    "    unique_embeddings = set()\n",
    "    output_file_name = f\"SkipGram_Disease{os.path.basename(csv_file_path).replace('disease_annotations', '').replace('.csv', '.txt')}\"\n",
    "    output_file_path = os.path.join(output_directory, output_file_name)\n",
    "\n",
    "    with open(output_file_path, 'w') as text_file:\n",
    "        for _, row in filtered_annotations.iterrows():\n",
    "            phrase = row['annotation_text']\n",
    "            mn_value = row['MN']\n",
    "            phrase_words = nltk.word_tokenize(phrase)\n",
    "            phrase_embeddings = [model.wv[word].tolist() for word in phrase_words if word in model.wv]\n",
    "\n",
    "            if phrase_embeddings:\n",
    "                # Perform average pooling\n",
    "                avg_embedding = [round(sum(vec) / len(vec), 6) for vec in zip(*phrase_embeddings)]\n",
    "                # Convert embedding list to string representation\n",
    "                embedding_str = ', '.join(map(str, avg_embedding))\n",
    "\n",
    "                # Write to file\n",
    "                text_file.write(f\"Phrase:{phrase}, MN:{mn_value}, Embedding: [{embedding_str}]\\n\")\n",
    "\n",
    "    print(f\"Embeddings saved in {output_file_path}\")\n",
    "\n",
    "# Define the directory containing the JSON and CSV files\n",
    "json_dir = \"split_pubtator\"\n",
    "csv_dir = \"csvfiles\"\n",
    "\n",
    "# Make sure the output directory exists\n",
    "output_dir = \"gen_wordembeddings/SkipGram/Disease\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Process each JSON file and its corresponding disease annotations CSV file, skipping files ending with \"_9\"\n",
    "for file_name in os.listdir(json_dir):\n",
    "    if file_name.endswith(\".json\"):\n",
    "        json_file_path = os.path.join(json_dir, file_name)\n",
    "        csv_file_name = f\"disease_annotations_{file_name.replace('proper_pubtator_', '').replace('.json', '')}.csv\"\n",
    "        csv_file_path = os.path.join(csv_dir, csv_file_name)\n",
    "        if os.path.exists(csv_file_path):\n",
    "            process_files(json_file_path, csv_file_path, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a02eb5-cad3-49c2-b728-65f0c8582d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "# Path to the directory containing the text files\n",
    "input_directory = 'gen_wordembeddings/SkipGram/Disease/'\n",
    "output_file_path = 'gen_wordembeddings/SkipGram/Disease/SkipGram_Disease_embeddings_combined.txt'  # Path for the output file\n",
    "\n",
    "# Use glob to match all '.txt' files in the directory\n",
    "text_files = glob.glob(os.path.join(input_directory, '*.txt'))\n",
    "\n",
    "# Open the output file in write mode\n",
    "with open(output_file_path, 'w') as outfile:\n",
    "    # Iterate over each file path in the list\n",
    "    for text_file_path in text_files:\n",
    "        # Open each file for reading\n",
    "        with open(text_file_path, 'r') as infile:\n",
    "            # Write its contents to the output file\n",
    "            outfile.write(infile.read())\n",
    "            # Optionally write a newline between the contents of each file\n",
    "            outfile.write('\\n')\n",
    "\n",
    "print(f'All text files from {input_directory} have been combined into {output_file_path}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041aaa21-36c0-42af-a484-5f049783ef75",
   "metadata": {},
   "source": [
    "# Gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaeb0857-d45a-4ae0-9c9b-0150e5ab2493",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "def process_files(json_file_path, csv_file_path, output_directory):\n",
    "    # Load JSON data\n",
    "    with open(json_file_path, 'r') as json_file:\n",
    "        json_data = json.load(json_file)\n",
    "\n",
    "    # Load gene annotations from CSV\n",
    "    annotations_df = pd.read_csv(csv_file_path)\n",
    "    annotations_df['annotation_text'] = annotations_df['annotation_text'].astype(str)\n",
    "\n",
    "    # Extract unique words from JSON data\n",
    "    all_words = set()\n",
    "    for entry in json_data:\n",
    "        if \"ARTICLE\" in entry and \"TEXT\" in entry[\"ARTICLE\"]:\n",
    "            text = entry[\"ARTICLE\"][\"TEXT\"]\n",
    "            words = nltk.word_tokenize(text)\n",
    "            all_words.update(words)\n",
    "\n",
    "    # Tokenize each entry in the 'annotation_text' column\n",
    "    annotations_df['annotation_words'] = annotations_df['annotation_text'].apply(nltk.word_tokenize)\n",
    "\n",
    "    # Flatten the list of tokenized words\n",
    "    all_annotation_words = [word for words_list in annotations_df['annotation_words'] for word in words_list]\n",
    "\n",
    "    # Filter annotations based on unique words\n",
    "    filtered_annotations = annotations_df[annotations_df['annotation_words'].apply(lambda x: any(word in all_words for word in x))]\n",
    "\n",
    "    # Train Word2Vec model (SkipGram)\n",
    "    model = Word2Vec(sentences=[all_annotation_words], vector_size=250, window=6, min_count=1, sg=1)\n",
    "\n",
    "    # Get unique phrases and their embeddings\n",
    "    unique_embeddings = set()\n",
    "    output_file_name = f\"SkipGram_Gene_{os.path.basename(csv_file_path).replace('gene_annotations_', '').replace('.csv', '.txt')}\"\n",
    "    output_file_path = os.path.join(output_directory, output_file_name)\n",
    "\n",
    "    with open(output_file_path, 'w') as text_file:\n",
    "        for phrase in set(filtered_annotations['annotation_text']):\n",
    "            phrase_words = nltk.word_tokenize(phrase)\n",
    "            phrase_embeddings = [model.wv[word].tolist() for word in phrase_words if word in model.wv]\n",
    "            \n",
    "            if phrase_embeddings:\n",
    "                avg_embedding = [round(sum(vec) / len(vec), 6) for vec in zip(*phrase_embeddings)]\n",
    "                if tuple(avg_embedding) not in unique_embeddings:\n",
    "                    unique_embeddings.add(tuple(avg_embedding))\n",
    "                    embedding_str = ', '.join(map(str, avg_embedding))\n",
    "                    text_file.write(f\"Gene: {phrase}, Embedding: [{embedding_str}]\\n\")\n",
    "\n",
    "# Define directories\n",
    "json_dir = \"split_pubtator\"\n",
    "csv_dir = \"csvfiles\"\n",
    "output_dir = \"gen_wordembeddings/SkipGram/Gene\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Process each JSON file and its corresponding gene annotations CSV file\n",
    "for file_name in os.listdir(json_dir):\n",
    "    if file_name.endswith(\".json\"):\n",
    "        json_file_path = os.path.join(json_dir, file_name)\n",
    "        csv_file_name = f\"gene_annotations_{file_name.replace('proper_pubtator_', '').replace('.json', '')}.csv\"\n",
    "        csv_file_path = os.path.join(csv_dir, csv_file_name)\n",
    "        if os.path.exists(csv_file_path):\n",
    "            process_files(json_file_path, csv_file_path, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28254bb9-a24d-4bbb-a1e0-9f87be23b65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "# Path to the directory containing the text files\n",
    "input_directory = 'gen_wordembeddings/SkipGram/Gene'\n",
    "output_file_path = 'gen_wordembeddings/SkipGram/Gene/SkipGram_gene_embeddings_combined.txt'  # Path for the output file\n",
    "\n",
    "# Use glob to match all '.txt' files in the directory\n",
    "text_files = glob.glob(os.path.join(input_directory, '*.txt'))\n",
    "\n",
    "# Open the output file in write mode\n",
    "with open(output_file_path, 'w') as outfile:\n",
    "    # Iterate over each file path in the list\n",
    "    for text_file_path in text_files:\n",
    "        # Open each file for reading\n",
    "        with open(text_file_path, 'r') as infile:\n",
    "            # Write its contents to the output file\n",
    "            outfile.write(infile.read())\n",
    "            # Optionally write a newline between the contents of each file\n",
    "            outfile.write('\\n')\n",
    "\n",
    "print(f'All text files from {input_directory} have been combined into {output_file_path}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3596a028-4acf-4c92-9ce8-9b95c209c1af",
   "metadata": {},
   "source": [
    "# Chemical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95460c6-a228-486d-aab7-cd87483a92f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "def process_files(json_file_path, csv_file_path, output_directory):\n",
    "    # Load JSON data\n",
    "    with open(json_file_path, 'r') as json_file:\n",
    "        json_data = json.load(json_file)\n",
    "\n",
    "    # Load chemical annotations from CSV\n",
    "    annotations_df = pd.read_csv(csv_file_path)\n",
    "\n",
    "    # Convert non-string values in 'annotation_text' column to strings\n",
    "    annotations_df['annotation_text'] = annotations_df['annotation_text'].astype(str)\n",
    "\n",
    "    # Extract unique words from the JSON data\n",
    "    all_words = set()\n",
    "    for entry in json_data:\n",
    "        text = entry.get(\"ARTICLE\", {}).get(\"TEXT\", \"\")\n",
    "        words = nltk.word_tokenize(text)\n",
    "        all_words.update(words)\n",
    "\n",
    "    # Tokenize each entry in the 'annotation_text' column\n",
    "    annotations_df['annotation_words'] = annotations_df['annotation_text'].apply(nltk.word_tokenize)\n",
    "\n",
    "    # Filter annotations based on unique words\n",
    "    filtered_annotations = annotations_df[annotations_df['annotation_words'].apply(lambda x: any(word in all_words for word in x))]\n",
    "\n",
    "    # Train Word2Vec model (SkipGram)\n",
    "    model = Word2Vec(sentences=[list(all_words)], vector_size=250, window=6, min_count=1, sg=1)\n",
    "\n",
    "    # Get unique phrases and their embeddings using average pooling\n",
    "    unique_embeddings = set()\n",
    "    output_file_name = f\"SkipGram_Chemical{os.path.basename(csv_file_path).replace('chemical_annotations', '').replace('.csv', '.txt')}\"\n",
    "    output_file_path = os.path.join(output_directory, output_file_name)\n",
    "\n",
    "    with open(output_file_path, 'w') as text_file:\n",
    "        for phrase in set(filtered_annotations['annotation_text']):\n",
    "            phrase_words = nltk.word_tokenize(phrase)\n",
    "            phrase_embeddings = [model.wv[word].tolist() for word in phrase_words if word in model.wv]\n",
    "\n",
    "            if phrase_embeddings:\n",
    "                # Perform average pooling\n",
    "                avg_embedding = [round(sum(vec) / len(vec), 6) for vec in zip(*phrase_embeddings)]\n",
    "                \n",
    "                # Convert embedding list to string representation\n",
    "                embedding_str = ', '.join(map(str, avg_embedding))\n",
    "\n",
    "                # Write\n",
    "                text_file.write(f\"Phrase:{phrase}, Embedding: [{embedding_str}]\\n\")\n",
    "\n",
    "    print(f\"Embeddings saved in {output_file_path}\")\n",
    "\n",
    "# Define the directory containing the JSON and CSV files\n",
    "json_dir = \"split_pubtator\"\n",
    "csv_dir = \"csvfiles\"\n",
    "output_directory = \"gen_wordembeddings/SkipGram/Chemical\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Process each JSON file and its corresponding chemical annotations CSV file, skipping files ending with \"_9\"\n",
    "for file_name in os.listdir(json_dir):\n",
    "    if file_name.endswith(\".json\"):\n",
    "        json_file_path = os.path.join(json_dir, file_name)\n",
    "        csv_file_name = f\"chemical_annotations_{file_name.replace('proper_pubtator_', '').replace('.json', '')}.csv\"\n",
    "        csv_file_path = os.path.join(csv_dir, csv_file_name)\n",
    "        if os.path.exists(csv_file_path):\n",
    "            process_files(json_file_path, csv_file_path, output_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e72743-56fd-414f-9e71-dc65af94d97e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53593fa-6430-4c4e-ba42-8912a49d8d88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee65b086-60b0-4a57-b945-e130629531fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
