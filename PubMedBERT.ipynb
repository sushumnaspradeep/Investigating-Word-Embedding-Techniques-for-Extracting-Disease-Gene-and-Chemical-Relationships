{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "553a3a8f-561a-4ddd-9503-77feaf95fb71",
   "metadata": {},
   "source": [
    "# Disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10621799-b950-40ea-914b-a8e22d6e12a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import os\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "from tqdm import tqdm\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "def process_files(json_file_path, csv_file_path, output_directory):\n",
    "    # Load JSON data\n",
    "    with open(json_file_path, 'r') as json_file:\n",
    "        json_data = json.load(json_file)\n",
    "\n",
    "    # Load disease annotations from CSV\n",
    "    annotations_df = pd.read_csv(csv_file_path)\n",
    "\n",
    "    # Convert non-string values in 'annotation_text' column to strings\n",
    "    annotations_df['annotation_text'] = annotations_df['annotation_text'].astype(str)\n",
    "\n",
    "    # Extract unique words from the JSON data\n",
    "    all_words = set()\n",
    "    for entry in json_data:\n",
    "        text = entry.get(\"ARTICLE\", {}).get(\"TEXT\", \"\")\n",
    "        words = nltk.word_tokenize(text)\n",
    "        all_words.update(words)\n",
    "\n",
    "    # Tokenize each entry in the 'annotation_text' column\n",
    "    annotations_df['annotation_words'] = annotations_df['annotation_text'].apply(nltk.word_tokenize)\n",
    "\n",
    "    # Filter annotations based on unique words\n",
    "    filtered_annotations = annotations_df[annotations_df['annotation_words'].apply(lambda x: any(word in all_words for word in x))]\n",
    "\n",
    "    # Load pre-trained PubMedBERT model and tokenizer\n",
    "    config = BertConfig.from_pretrained('microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext', output_hidden_states=True)\n",
    "    tokenizer = BertTokenizer.from_pretrained('microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext')\n",
    "    model = BertModel.from_pretrained('microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext', config=config)\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    # Create different output files for different types of embeddings\n",
    "    output_file_sum = os.path.join(output_directory, f\"PubMedBERT_Disease_Sum{os.path.basename(csv_file_path).replace('disease_annotations', '').replace('.csv', '.txt')}\")\n",
    "    output_file_last4 = [os.path.join(output_directory, f\"PubMedBERT_Disease_Layer_{i}{os.path.basename(csv_file_path).replace('disease_annotations', '').replace('.csv', '.txt')}\") for i in range(-4, 0)]\n",
    "\n",
    "    with open(output_file_sum, 'w') as text_file_sum:\n",
    "        text_files_last4 = [open(file_name, 'w') for file_name in output_file_last4]\n",
    "\n",
    "        for phrase in tqdm(set(filtered_annotations['annotation_text']), desc=\"Processing phrases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebb36a5-1daf-4ef2-98ef-37a1bfd76e37",
   "metadata": {},
   "source": [
    "## Chemical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8fe1a4-744a-4131-9eed-9221ca7e2aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import os\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "from tqdm import tqdm\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "def process_files(json_file_path, csv_file_path, output_directory):\n",
    "    # Load JSON data\n",
    "    with open(json_file_path, 'r') as json_file:\n",
    "        json_data = json.load(json_file)\n",
    "\n",
    "    # Load chemical annotations from CSV\n",
    "    annotations_df = pd.read_csv(csv_file_path)\n",
    "\n",
    "    # Convert non-string values in 'annotation_text' column to strings\n",
    "    annotations_df['annotation_text'] = annotations_df['annotation_text'].astype(str)\n",
    "\n",
    "    # Extract unique words from the JSON data\n",
    "    all_words = set()\n",
    "    for entry in json_data:\n",
    "        text = entry.get(\"ARTICLE\", {}).get(\"TEXT\", \"\")\n",
    "        words = nltk.word_tokenize(text)\n",
    "        all_words.update(words)\n",
    "\n",
    "    # Tokenize each entry in the 'annotation_text' column\n",
    "    annotations_df['annotation_words'] = annotations_df['annotation_text'].apply(nltk.word_tokenize)\n",
    "\n",
    "    # Filter annotations based on unique words\n",
    "    filtered_annotations = annotations_df[annotations_df['annotation_words'].apply(lambda x: any(word in all_words for word in x))]\n",
    "\n",
    "    # Load pre-trained PubMedBERT model and tokenizer\n",
    "    config = BertConfig.from_pretrained('microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext', output_hidden_states=True)\n",
    "    tokenizer = BertTokenizer.from_pretrained('microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext')\n",
    "    model = BertModel.from_pretrained('microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext', config=config)\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    # Create different output files for different types of embeddings\n",
    "    output_file_sum = os.path.join(output_directory, f\"PubMedBERT_Chemical_Sum{os.path.basename(csv_file_path).replace('chemical_annotations', '').replace('.csv', '.txt')}\")\n",
    "    output_file_last4 = [os.path.join(output_directory, f\"PubMedBERT_Chemical_Layer_{i}{os.path.basename(csv_file_path).replace('chemical_annotations', '').replace('.csv', '.txt')}\") for i in range(-4, 0)]\n",
    "\n",
    "    with open(output_file_sum, 'w') as text_file_sum:\n",
    "        text_files_last4 = [open(file_name, 'w') for file_name in output_file_last4]\n",
    "\n",
    "        for phrase in tqdm(set(filtered_annotations['annotation_text']), desc=\"Processing phrases\"):\n",
    "            # Tokenize and encode the phrase\n",
    "            inputs = tokenizer(phrase, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "\n",
    "            # Get the hidden states for the last 4 layers\n",
    "            hidden_states = outputs.hidden_states[-4:]\n",
    "\n",
    "            # Sum of the last 4 layers\n",
    "            sum_embedding = torch.sum(torch.stack(hidden_states), dim=0).squeeze(0)\n",
    "            sum_embedding = torch.sum(sum_embedding, dim=0).tolist()\n",
    "            embedding_str_sum = ', '.join(map(str, sum_embedding))\n",
    "            text_file_sum.write(f\"Phrase:{phrase}, Embedding: [{embedding_str_sum}]\\n\")\n",
    "\n",
    "            # Save each of the last 4 layers individually\n",
    "            for i, hidden_state in enumerate(hidden_states):\n",
    "                layer_embedding = torch.mean(hidden_state.squeeze(0), dim=0).tolist()\n",
    "                embedding_str_layer = ', '.join(map(str, layer_embedding))\n",
    "                text_files_last4[i].write(f\"Phrase:{phrase}, Embedding: [{embedding_str_layer}]\\n\")\n",
    "\n",
    "        # Close individual layer files\n",
    "        for text_file in text_files_last4:\n",
    "            text_file.close()\n",
    "\n",
    "    print(f\"Summation embeddings saved in {output_file_sum}\")\n",
    "    print(f\"Individual layer embeddings saved in {output_directory}\")\n",
    "\n",
    "# Define the directory containing the JSON and CSV files\n",
    "json_dir = \"split_pubtator\"\n",
    "csv_dir = \"csvfiles\"\n",
    "output_directory = \"3gen_wordembeddings/PubMedBERT/Chemical\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Process each JSON file and its corresponding chemical annotations CSV file, processing only those ending with \".json\"\n",
    "for file_name in tqdm(os.listdir(json_dir), desc=\"Processing files\"):\n",
    "    if file_name.endswith(\".json\"):\n",
    "        json_file_path = os.path.join(json_dir, file_name)\n",
    "        csv_file_name = f\"chemical_annotations_{file_name.replace('proper_pubtator_', '').replace('.json', '')}.csv\"\n",
    "        csv_file_path = os.path.join(csv_dir, csv_file_name)\n",
    "        if os.path.exists(csv_file_path):\n",
    "            process_files(json_file_path, csv_file_path, output_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ce68fa-362f-4f27-9ef0-d953761d6ef5",
   "metadata": {},
   "source": [
    "## Gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70741a43-4e5d-499d-aa04-58893c9fa33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import os\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "from tqdm import tqdm\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "def process_files(json_file_path, csv_file_path, output_directory):\n",
    "    # Load JSON data\n",
    "    with open(json_file_path, 'r') as json_file:\n",
    "        json_data = json.load(json_file)\n",
    "\n",
    "    # Load gene annotations from CSV\n",
    "    annotations_df = pd.read_csv(csv_file_path)\n",
    "\n",
    "    # Convert non-string values in 'annotation_text' column to strings\n",
    "    annotations_df['annotation_text'] = annotations_df['annotation_text'].astype(str)\n",
    "\n",
    "    # Extract unique words from the JSON data\n",
    "    all_words = set()\n",
    "    for entry in json_data:\n",
    "        text = entry.get(\"ARTICLE\", {}).get(\"TEXT\", \"\")\n",
    "        words = nltk.word_tokenize(text)\n",
    "        all_words.update(words)\n",
    "\n",
    "    # Tokenize each entry in the 'annotation_text' column\n",
    "    annotations_df['annotation_words'] = annotations_df['annotation_text'].apply(nltk.word_tokenize)\n",
    "\n",
    "    # Filter annotations based on unique words\n",
    "    filtered_annotations = annotations_df[annotations_df['annotation_words'].apply(lambda x: any(word in all_words for word in x))]\n",
    "\n",
    "    # Load pre-trained PubMedBERT model and tokenizer\n",
    "    config = BertConfig.from_pretrained('microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext', output_hidden_states=True)\n",
    "    tokenizer = BertTokenizer.from_pretrained('microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext')\n",
    "    model = BertModel.from_pretrained('microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext', config=config)\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    # Create different output files for different types of embeddings\n",
    "    output_file_sum = os.path.join(output_directory, f\"PubMedBERT_Gene_Sum{os.path.basename(csv_file_path).replace('gene_annotations', '').replace('.csv', '.txt')}\")\n",
    "    output_file_last4 = [os.path.join(output_directory, f\"PubMedBERT_Gene_Layer_{i}{os.path.basename(csv_file_path).replace('gene_annotations', '').replace('.csv', '.txt')}\") for i in range(-4, 0)]\n",
    "\n",
    "    with open(output_file_sum, 'w') as text_file_sum:\n",
    "        text_files_last4 = [open(file_name, 'w') for file_name in output_file_last4]\n",
    "\n",
    "        for phrase in tqdm(set(filtered_annotations['annotation_text']), desc=\"Processing phrases\"):\n",
    "            # Tokenize and encode the phrase\n",
    "            inputs = tokenizer(phrase, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "\n",
    "            # Get the hidden states for the last 4 layers\n",
    "            hidden_states = outputs.hidden_states[-4:]\n",
    "\n",
    "            # Sum of the last 4 layers\n",
    "            sum_embedding = torch.sum(torch.stack(hidden_states), dim=0).squeeze(0)\n",
    "            sum_embedding = torch.sum(sum_embedding, dim=0).tolist()\n",
    "            embedding_str_sum = ', '.join(map(str, sum_embedding))\n",
    "            text_file_sum.write(f\"Phrase:{phrase}, Embedding: [{embedding_str_sum}]\\n\")\n",
    "\n",
    "            # Save each of the last 4 layers individually\n",
    "            for i, hidden_state in enumerate(hidden_states):\n",
    "                layer_embedding = torch.mean(hidden_state.squeeze(0), dim=0).tolist()\n",
    "                embedding_str_layer = ', '.join(map(str, layer_embedding))\n",
    "                text_files_last4[i].write(f\"Phrase:{phrase}, Embedding: [{embedding_str_layer}]\\n\")\n",
    "\n",
    "        # Close individual layer files\n",
    "        for text_file in text_files_last4:\n",
    "            text_file.close()\n",
    "\n",
    "    print(f\"Summation embeddings saved in {output_file_sum}\")\n",
    "    print(f\"Individual layer embeddings saved in {output_directory}\")\n",
    "\n",
    "# Define the directory containing the JSON and CSV files\n",
    "json_dir = \"split_pubtator\"\n",
    "csv_dir = \"csvfiles\"\n",
    "output_directory = \"3gen_wordembeddings/PubMedBERT/Gene\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Process each JSON file and its corresponding gene annotations CSV file, processing only those ending with \".json\"\n",
    "for file_name in tqdm(os.listdir(json_dir), desc=\"Processing files\"):\n",
    "    if file_name.endswith(\".json\"):\n",
    "        json_file_path = os.path.join(json_dir, file_name)\n",
    "        csv_file_name = f\"gene_annotations_{file_name.replace('proper_pubtator_', '').replace('.json', '')}.csv\"\n",
    "        csv_file_path = os.path.join(csv_dir, csv_file_name)\n",
    "        if os.path.exists(csv_file_path):\n",
    "            process_files(json_file_path, csv_file_path, output_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70901415-1dc7-4453-862c-baa352678156",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
