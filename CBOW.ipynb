{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c35470b0-d579-42a5-8a3d-34e46e212b6d",
   "metadata": {},
   "source": [
    "# Disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b75567-c0c8-46d9-9e94-014aa5fcb9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "def process_files(json_file_path, csv_file_path, output_directory):\n",
    "    # Load JSON data\n",
    "    with open(json_file_path, 'r') as json_file:\n",
    "        json_data = json.load(json_file)\n",
    "\n",
    "    # Load disease annotations from CSV\n",
    "    annotations_df = pd.read_csv(csv_file_path)\n",
    "\n",
    "    # Extract unique words from the JSON data\n",
    "    all_words = set()\n",
    "    for entry in json_data:\n",
    "        text = entry.get(\"ARTICLE\", {}).get(\"TEXT\", \"\")\n",
    "        words = nltk.word_tokenize(text)\n",
    "        all_words.update(words)\n",
    "\n",
    "    # Tokenize each entry in the 'annotation_text' column, ensuring that each element is a string\n",
    "    annotations_df['annotation_words'] = annotations_df['annotation_text'].apply(\n",
    "        lambda x: nltk.word_tokenize(str(x)) if pd.notnull(x) else []\n",
    "    )\n",
    "\n",
    "    # Filter annotations based on unique words\n",
    "    filtered_annotations = annotations_df[annotations_df['annotation_words'].apply(lambda x: any(word in all_words for word in x))]\n",
    "\n",
    "    # Train Word2Vec model (CBOW)\n",
    "    model = Word2Vec(sentences=[list(all_words)], vector_size=250, window=6, min_count=1, sg=0)\n",
    "\n",
    "    # Get unique phrases, their MN values, and embeddings using average pooling\n",
    "    unique_embeddings = set()\n",
    "    output_file_name = f\"CBOW_Disease{os.path.basename(csv_file_path).replace('disease_annotations', '').replace('.csv', '.txt')}\"\n",
    "    output_file_path = os.path.join(output_directory, output_file_name)\n",
    "\n",
    "    with open(output_file_path, 'w') as text_file:\n",
    "        for _, row in filtered_annotations.iterrows():\n",
    "            phrase = row['annotation_text']\n",
    "            mn_value = row['MN']\n",
    "            phrase_words = nltk.word_tokenize(phrase)\n",
    "            phrase_embeddings = [model.wv[word].tolist() for word in phrase_words if word in model.wv]\n",
    "\n",
    "            if phrase_embeddings:\n",
    "                # Perform average pooling\n",
    "                avg_embedding = [round(sum(vec) / len(vec), 6) for vec in zip(*phrase_embeddings)]\n",
    "                # Convert embedding list to string representation\n",
    "                embedding_str = ', '.join(map(str, avg_embedding))\n",
    "\n",
    "                # Write to file\n",
    "                text_file.write(f\"Phrase:{phrase}, MN:{mn_value}, Embedding: [{embedding_str}]\\n\")\n",
    "\n",
    "    print(f\"Embeddings saved in {output_file_path}\")\n",
    "\n",
    "# Define the directory containing the JSON and CSV files\n",
    "json_dir = \"split_pubtator\"\n",
    "csv_dir = \"csvfiles\"\n",
    "\n",
    "# Make sure the output directory exists\n",
    "output_dir = \"gen_wordembeddings/CBOW/Disease\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Process each JSON file and its corresponding disease annotations CSV file\n",
    "# Process each JSON file and its corresponding disease annotations CSV file, skipping files ending with \"_9\"\n",
    "for file_name in os.listdir(json_dir):\n",
    "    if file_name.endswith(\".json\"):\n",
    "        json_file_path = os.path.join(json_dir, file_name)\n",
    "        csv_file_name = f\"disease_annotations_{file_name.replace('proper_pubtator_', '').replace('.json', '')}.csv\"\n",
    "        csv_file_path = os.path.join(csv_dir, csv_file_name)\n",
    "        if os.path.exists(csv_file_path):\n",
    "            process_files(json_file_path, csv_file_path, output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bc8057-9d67-44fc-a4e6-488448e15b03",
   "metadata": {},
   "source": [
    "Note: The parameters can be adjusted based on the required window size and vector size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de975680-1e7f-4225-a4ef-b6ee1addb647",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "# Path to the directory containing the text files\n",
    "input_directory = 'gen_wordembeddings/CBOW/Disease/'\n",
    "output_file_path = 'gen_wordembeddings/CBOW/Disease/CBOW_Disease_embeddings_combined.txt'  # Path for the output file\n",
    "\n",
    "# Use glob to match all '.txt' files in the directory\n",
    "text_files = glob.glob(os.path.join(input_directory, '*.txt'))\n",
    "\n",
    "# Open the output file in write mode\n",
    "with open(output_file_path, 'w') as outfile:\n",
    "    # Iterate over each file path in the list\n",
    "    for text_file_path in text_files:\n",
    "        # Open each file for reading\n",
    "        with open(text_file_path, 'r') as infile:\n",
    "            # Write its contents to the output file\n",
    "            outfile.write(infile.read())\n",
    "            # Optionally write a newline between the contents of each file\n",
    "            outfile.write('\\n')\n",
    "\n",
    "print(f'All text files from {input_directory} have been combined into {output_file_path}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83f6625-9755-42a3-bbd8-61d37fa1889a",
   "metadata": {},
   "source": [
    "Note: Combine all the word embeddings of a particular model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac007c87-8eda-4616-81d9-bcedd0c83e1f",
   "metadata": {},
   "source": [
    "# Gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f602282e-d51e-42a6-8dfd-017e790d3585",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "def process_files(json_file_path, csv_file_path, output_directory):\n",
    "    # Load JSON data\n",
    "    with open(json_file_path, 'r') as json_file:\n",
    "        json_data = json.load(json_file)\n",
    "\n",
    "    # Load gene annotations from CSV\n",
    "    annotations_df = pd.read_csv(csv_file_path)\n",
    "    annotations_df['annotation_text'] = annotations_df['annotation_text'].astype(str)\n",
    "\n",
    "    # Extract unique words from JSON data\n",
    "    all_words = set()\n",
    "    for entry in json_data:\n",
    "        if \"ARTICLE\" in entry and \"TEXT\" in entry[\"ARTICLE\"]:\n",
    "            text = entry[\"ARTICLE\"][\"TEXT\"]\n",
    "            words = nltk.word_tokenize(text)\n",
    "            all_words.update(words)\n",
    "\n",
    "    # Tokenize each entry in the 'annotation_text' column\n",
    "    annotations_df['annotation_words'] = annotations_df['annotation_text'].apply(nltk.word_tokenize)\n",
    "\n",
    "    # Flatten the list of tokenized words\n",
    "    all_annotation_words = [word for words_list in annotations_df['annotation_words'] for word in words_list]\n",
    "\n",
    "    # Filter annotations based on unique words\n",
    "    filtered_annotations = annotations_df[annotations_df['annotation_words'].apply(lambda x: any(word in all_words for word in x))]\n",
    "\n",
    "    # Train Word2Vec model (CBOW)\n",
    "    model = Word2Vec(sentences=[all_annotation_words], vector_size=250, window=6, min_count=1, sg=0)\n",
    "\n",
    "    # Get unique phrases and their embeddings\n",
    "    unique_embeddings = set()\n",
    "    output_file_name = f\"CBOW_Gene_{os.path.basename(csv_file_path).replace('gene_annotations_', '').replace('.csv', '.txt')}\"\n",
    "    output_file_path = os.path.join(output_directory, output_file_name)\n",
    "\n",
    "    with open(output_file_path, 'w') as text_file:\n",
    "        for phrase in set(filtered_annotations['annotation_text']):\n",
    "            phrase_words = nltk.word_tokenize(phrase)\n",
    "            phrase_embeddings = [model.wv[word].tolist() for word in phrase_words if word in model.wv]\n",
    "            \n",
    "            if phrase_embeddings:\n",
    "                avg_embedding = [round(sum(vec) / len(vec), 6) for vec in zip(*phrase_embeddings)]\n",
    "                if tuple(avg_embedding) not in unique_embeddings:\n",
    "                    unique_embeddings.add(tuple(avg_embedding))\n",
    "                    embedding_str = ', '.join(map(str, avg_embedding))\n",
    "                    text_file.write(f\"Gene: {phrase}, Embedding: [{embedding_str}]\\n\")\n",
    "\n",
    "# Define directories\n",
    "json_dir = \"split_pubtator\"\n",
    "csv_dir = \"csvfiles\"\n",
    "output_dir = \"gen_wordembeddings/CBOW/Gene\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Process each JSON file and its corresponding gene annotations CSV file\n",
    "for file_name in os.listdir(json_dir):\n",
    "    if file_name.endswith(\".json\"):\n",
    "        json_file_path = os.path.join(json_dir, file_name)\n",
    "        csv_file_name = f\"gene_annotations_{file_name.replace('proper_pubtator_', '').replace('.json', '')}.csv\"\n",
    "        csv_file_path = os.path.join(csv_dir, csv_file_name)\n",
    "        if os.path.exists(csv_file_path):\n",
    "            process_files(json_file_path, csv_file_path, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2f2f72-a113-4dfc-a654-6d385e3afed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "# Path to the directory containing the text files\n",
    "input_directory = 'gen_wordembeddings/CBOW/Gene'\n",
    "output_file_path = 'gen_wordembeddings/CBOW/Gene/CBOW_gene_embeddings_combined.txt'  # Path for the output file\n",
    "\n",
    "# Use glob to match all '.txt' files in the directory\n",
    "text_files = glob.glob(os.path.join(input_directory, '*.txt'))\n",
    "\n",
    "# Open the output file in write mode\n",
    "with open(output_file_path, 'w') as outfile:\n",
    "    # Iterate over each file path in the list\n",
    "    for text_file_path in text_files:\n",
    "        # Open each file for reading\n",
    "        with open(text_file_path, 'r') as infile:\n",
    "            # Write its contents to the output file\n",
    "            outfile.write(infile.read())\n",
    "            # Optionally write a newline between the contents of each file\n",
    "            outfile.write('\\n')\n",
    "\n",
    "print(f'All text files from {input_directory} have been combined into {output_file_path}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2ad0dc-59b4-4a1e-88ab-bd7bdcf45e07",
   "metadata": {},
   "source": [
    "# Chemical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b33747-ddd7-4e9e-b600-426e8fa8a02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "def process_files(json_file_path, csv_file_path, output_directory):\n",
    "    # Load JSON data\n",
    "    with open(json_file_path, 'r') as json_file:\n",
    "        json_data = json.load(json_file)\n",
    "\n",
    "    # Load chemical annotations from CSV\n",
    "    annotations_df = pd.read_csv(csv_file_path)\n",
    "\n",
    "    # Convert non-string values in 'annotation_text' column to strings\n",
    "    annotations_df['annotation_text'] = annotations_df['annotation_text'].astype(str)\n",
    "\n",
    "    # Extract unique words from the JSON data\n",
    "    all_words = set()\n",
    "    for entry in json_data:\n",
    "        text = entry.get(\"ARTICLE\", {}).get(\"TEXT\", \"\")\n",
    "        words = nltk.word_tokenize(text)\n",
    "        all_words.update(words)\n",
    "\n",
    "    # Tokenize each entry in the 'annotation_text' column\n",
    "    annotations_df['annotation_words'] = annotations_df['annotation_text'].apply(nltk.word_tokenize)\n",
    "\n",
    "    # Filter annotations based on unique words\n",
    "    filtered_annotations = annotations_df[annotations_df['annotation_words'].apply(lambda x: any(word in all_words for word in x))]\n",
    "\n",
    "    # Train Word2Vec model (CBOW)\n",
    "    model = Word2Vec(sentences=[list(all_words)], vector_size=250, window=6, min_count=1, sg=0)\n",
    "\n",
    "    # Get unique phrases and their embeddings using average pooling\n",
    "    unique_embeddings = set()\n",
    "    output_file_name = f\"CBOW_Chemical{os.path.basename(csv_file_path).replace('chemical_annotations', '').replace('.csv', '.txt')}\"\n",
    "    output_file_path = os.path.join(output_directory, output_file_name)\n",
    "\n",
    "    with open(output_file_path, 'w') as text_file:\n",
    "        for phrase in set(filtered_annotations['annotation_text']):\n",
    "            phrase_words = nltk.word_tokenize(phrase)\n",
    "            phrase_embeddings = [model.wv[word].tolist() for word in phrase_words if word in model.wv]\n",
    "\n",
    "            if phrase_embeddings:\n",
    "                # Perform average pooling\n",
    "                avg_embedding = [round(sum(vec) / len(vec), 6) for vec in zip(*phrase_embeddings)]\n",
    "                \n",
    "                # Convert embedding list to string representation\n",
    "                embedding_str = ', '.join(map(str, avg_embedding))\n",
    "\n",
    "                # Write\n",
    "                text_file.write(f\"Phrase:{phrase}, Embedding: [{embedding_str}]\\n\")\n",
    "\n",
    "    print(f\"Embeddings saved in {output_file_path}\")\n",
    "\n",
    "# Define the directory containing the JSON and CSV files\n",
    "json_dir = \"split_pubtator\"\n",
    "csv_dir = \"csvfiles\"\n",
    "output_directory = \"gen_wordembeddings/CBOW/Chemical\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Process each JSON file and its corresponding chemical annotations CSV file, skipping files ending with \"_9\"\n",
    "for file_name in os.listdir(json_dir):\n",
    "    if file_name.endswith(\".json\"):\n",
    "        json_file_path = os.path.join(json_dir, file_name)\n",
    "        csv_file_name = f\"chemical_annotations_{file_name.replace('proper_pubtator_', '').replace('.json', '')}.csv\"\n",
    "        csv_file_path = os.path.join(csv_dir, csv_file_name)\n",
    "        if os.path.exists(csv_file_path):\n",
    "            process_files(json_file_path, csv_file_path, output_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7a7719-1181-4dbb-b0d5-3d6ea9e668de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "# Path to the directory containing the text files\n",
    "input_directory = 'gen_wordembeddings/CBOW/Chemical/'\n",
    "output_file_path = 'gen_wordembeddings/CBOW/Chemical/CBOW_chemical_embeddings_combined.txt'  # Path for the output file\n",
    "\n",
    "# Use glob to match all '.txt' files in the directory\n",
    "text_files = glob.glob(os.path.join(input_directory, '*.txt'))\n",
    "\n",
    "# Open the output file in write mode\n",
    "with open(output_file_path, 'w') as outfile:\n",
    "    # Iterate over each file path in the list\n",
    "    for text_file_path in text_files:\n",
    "        # Open each file for reading\n",
    "        with open(text_file_path, 'r') as infile:\n",
    "            # Write its contents to the output file\n",
    "            outfile.write(infile.read())\n",
    "            # Optionally write a newline between the contents of each file\n",
    "            outfile.write('\\n')\n",
    "\n",
    "print(f'All text files from {input_directory} have been combined into {output_file_path}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d946725-f18e-439f-be4b-77bfd1d93d28",
   "metadata": {},
   "source": [
    "Note : This file shows the code to create word embeddings using CBOW models for Disease, Gene and Chemicals. We can use various window size and vector size to create the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3321d10-a794-4563-aef0-7d87d68254b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
