{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dedfef86-baaa-4218-a9e1-1e80d01ef37b",
   "metadata": {},
   "source": [
    "# Disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da92fea-8dee-407a-9f12-c5afc8d35184",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import os\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "from tqdm import tqdm\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "def process_files(json_file_path, csv_file_path, output_directory):\n",
    "    # Load JSON data\n",
    "    with open(json_file_path, 'r') as json_file:\n",
    "        json_data = json.load(json_file)\n",
    "\n",
    "    # Load disease annotations from CSV\n",
    "    annotations_df = pd.read_csv(csv_file_path)\n",
    "\n",
    "    # Convert non-string values in 'annotation_text' column to strings\n",
    "    annotations_df['annotation_text'] = annotations_df['annotation_text'].astype(str)\n",
    "\n",
    "    # Extract unique words from the JSON data\n",
    "    all_words = set()\n",
    "    for entry in json_data:\n",
    "        text = entry.get(\"ARTICLE\", {}).get(\"TEXT\", \"\")\n",
    "        words = nltk.word_tokenize(text)\n",
    "        all_words.update(words)\n",
    "\n",
    "    # Tokenize each entry in the 'annotation_text' column\n",
    "    annotations_df['annotation_words'] = annotations_df['annotation_text'].apply(nltk.word_tokenize)\n",
    "\n",
    "    # Filter annotations based on unique words\n",
    "    filtered_annotations = annotations_df[annotations_df['annotation_words'].apply(lambda x: any(word in all_words for word in x))]\n",
    "\n",
    "    # Load pre-trained BioBERT model and tokenizer\n",
    "    config = BertConfig.from_pretrained('dmis-lab/biobert-base-cased-v1.1', output_hidden_states=True)\n",
    "    tokenizer = BertTokenizer.from_pretrained('dmis-lab/biobert-base-cased-v1.1')\n",
    "    model = BertModel.from_pretrained('dmis-lab/biobert-base-cased-v1.1', config=config)\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    # Create different output files for different types of embeddings\n",
    "    output_file_sum = os.path.join(output_directory, f\"BioBERT_Disease_Sum{os.path.basename(csv_file_path).replace('disease_annotations', '').replace('.csv', '.txt')}\")\n",
    "    output_file_last4 = [os.path.join(output_directory, f\"BioBERT_Disease_Layer_{i}{os.path.basename(csv_file_path).replace('disease_annotations', '').replace('.csv', '.txt')}\") for i in range(-4, 0)]\n",
    "\n",
    "    with open(output_file_sum, 'w') as text_file_sum:\n",
    "        text_files_last4 = [open(file_name, 'w') for file_name in output_file_last4]\n",
    "\n",
    "        for phrase in tqdm(set(filtered_annotations['annotation_text']), desc=\"Processing phrases\"):\n",
    "            # Tokenize and encode the phrase\n",
    "            inputs = tokenizer(phrase, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "\n",
    "            # Get the hidden states for the last 4 layers\n",
    "            hidden_states = outputs.hidden_states[-4:]\n",
    "\n",
    "            # Sum of the last 4 layers\n",
    "            sum_embedding = torch.sum(torch.stack(hidden_states), dim=0).squeeze(0)\n",
    "            sum_embedding = torch.sum(sum_embedding, dim=0).tolist()\n",
    "            embedding_str_sum = ', '.join(map(str, sum_embedding))\n",
    "            text_file_sum.write(f\"Phrase:{phrase}, Embedding: [{embedding_str_sum}]\\n\")\n",
    "\n",
    "            # Save each of the last 4 layers individually\n",
    "            for i, hidden_state in enumerate(hidden_states):\n",
    "                layer_embedding = torch.mean(hidden_state.squeeze(0), dim=0).tolist()\n",
    "                embedding_str_layer = ', '.join(map(str, layer_embedding))\n",
    "                text_files_last4[i].write(f\"Phrase:{phrase}, Embedding: [{embedding_str_layer}]\\n\")\n",
    "\n",
    "        # Close individual layer files\n",
    "        for text_file in text_files_last4:\n",
    "            text_file.close()\n",
    "\n",
    "    print(f\"Summation embeddings saved in {output_file_sum}\")\n",
    "    print(f\"Individual layer embeddings saved in {output_directory}\")\n",
    "\n",
    "# Define the directory containing the JSON and CSV files\n",
    "json_dir = \"split_pubtator\"\n",
    "csv_dir = \"csvfiles\"\n",
    "output_directory = \"3gen_wordembeddings/BioBERT/Disease\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Process each JSON file and its corresponding disease annotations CSV file, processing only those ending with \".json\"\n",
    "for file_name in tqdm(os.listdir(json_dir), desc=\"Processing files\"):\n",
    "    if file_name.endswith(\".json\"):\n",
    "        json_file_path = os.path.join(json_dir, file_name)\n",
    "        csv_file_name = f\"disease_annotations_{file_name.replace('proper_pubtator_', '').replace('.json', '')}.csv\"\n",
    "        csv_file_path = os.path.join(csv_dir, csv_file_name)\n",
    "        if os.path.exists(csv_file_path):\n",
    "            process_files(json_file_path, csv_file_path, output_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034719f4-9df6-445b-9259-72eb61bf5c47",
   "metadata": {},
   "source": [
    "##  Chemical "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07244316-6060-4234-95d9-aca5deab12d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import os\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "from tqdm import tqdm\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "def process_files(json_file_path, csv_file_path, output_directory):\n",
    "    # Load JSON data\n",
    "    with open(json_file_path, 'r') as json_file:\n",
    "        json_data = json.load(json_file)\n",
    "\n",
    "    # Load chemical annotations from CSV\n",
    "    annotations_df = pd.read_csv(csv_file_path)\n",
    "\n",
    "    # Convert non-string values in 'annotation_text' column to strings\n",
    "    annotations_df['annotation_text'] = annotations_df['annotation_text'].astype(str)\n",
    "\n",
    "    # Extract unique words from the JSON data\n",
    "    all_words = set()\n",
    "    for entry in json_data:\n",
    "        text = entry.get(\"ARTICLE\", {}).get(\"TEXT\", \"\")\n",
    "        words = nltk.word_tokenize(text)\n",
    "        all_words.update(words)\n",
    "\n",
    "    # Tokenize each entry in the 'annotation_text' column\n",
    "    annotations_df['annotation_words'] = annotations_df['annotation_text'].apply(nltk.word_tokenize)\n",
    "\n",
    "    # Filter annotations based on unique words\n",
    "    filtered_annotations = annotations_df[annotations_df['annotation_words'].apply(lambda x: any(word in all_words for word in x))]\n",
    "\n",
    "    # Load pre-trained BioBERT model and tokenizer\n",
    "    config = BertConfig.from_pretrained('dmis-lab/biobert-base-cased-v1.1', output_hidden_states=True)\n",
    "    tokenizer = BertTokenizer.from_pretrained('dmis-lab/biobert-base-cased-v1.1')\n",
    "    model = BertModel.from_pretrained('dmis-lab/biobert-base-cased-v1.1', config=config)\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    # Create different output files for different types of embeddings\n",
    "    output_file_sum = os.path.join(output_directory, f\"BioBERT_Chemical_Sum{os.path.basename(csv_file_path).replace('chemical_annotations', '').replace('.csv', '.txt')}\")\n",
    "    output_file_last4 = [os.path.join(output_directory, f\"BioBERT_Chemical_Layer_{i}{os.path.basename(csv_file_path).replace('chemical_annotations', '').replace('.csv', '.txt')}\") for i in range(-4, 0)]\n",
    "\n",
    "    with open(output_file_sum, 'w') as text_file_sum:\n",
    "        text_files_last4 = [open(file_name, 'w') for file_name in output_file_last4]\n",
    "\n",
    "        for phrase in tqdm(set(filtered_annotations['annotation_text']), desc=\"Processing phrases\"):\n",
    "            # Tokenize and encode the phrase\n",
    "            inputs = tokenizer(phrase, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "\n",
    "            # Get the hidden states for the last 4 layers\n",
    "            hidden_states = outputs.hidden_states[-4:]\n",
    "\n",
    "            # Sum of the last 4 layers\n",
    "            sum_embedding = torch.sum(torch.stack(hidden_states), dim=0).squeeze(0)\n",
    "            sum_embedding = torch.sum(sum_embedding, dim=0).tolist()\n",
    "            embedding_str_sum = ', '.join(map(str, sum_embedding))\n",
    "            text_file_sum.write(f\"Phrase:{phrase}, Embedding: [{embedding_str_sum}]\\n\")\n",
    "\n",
    "            # Save each of the last 4 layers individually\n",
    "            for i, hidden_state in enumerate(hidden_states):\n",
    "                layer_embedding = torch.mean(hidden_state.squeeze(0), dim=0).tolist()\n",
    "                embedding_str_layer = ', '.join(map(str, layer_embedding))\n",
    "                text_files_last4[i].write(f\"Phrase:{phrase}, Embedding: [{embedding_str_layer}]\\n\")\n",
    "\n",
    "        # Close individual layer files\n",
    "        for text_file in text_files_last4:\n",
    "            text_file.close()\n",
    "\n",
    "    print(f\"Summation embeddings saved in {output_file_sum}\")\n",
    "    print(f\"Individual layer embeddings saved in {output_directory}\")\n",
    "\n",
    "# Define the directory containing the JSON and CSV files\n",
    "json_dir = \"split_pubtator\"\n",
    "csv_dir = \"csvfiles\"\n",
    "output_directory = \"3gen_wordembeddings/BioBERT/Chemical\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Process each JSON file and its corresponding chemical annotations CSV file, processing only those ending with \".json\"\n",
    "for file_name in tqdm(os.listdir(json_dir), desc=\"Processing files\"):\n",
    "    if file_name.endswith(\".json\"):\n",
    "        json_file_path = os.path.join(json_dir, file_name)\n",
    "        csv_file_name = f\"chemical_annotations_{file_name.replace('proper_pubtator_', '').replace('.json', '')}.csv\"\n",
    "        csv_file_path = os.path.join(csv_dir, csv_file_name)\n",
    "        if os.path.exists(csv_file_path):\n",
    "            process_files(json_file_path, csv_file_path, output_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda2dbd4-3186-4725-a732-208a425c1b95",
   "metadata": {},
   "source": [
    "## Gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a2d2f0-87be-4525-b614-76c2173102a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import os\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "from tqdm import tqdm\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "def process_files(json_file_path, csv_file_path, output_directory):\n",
    "    # Load JSON data\n",
    "    with open(json_file_path, 'r') as json_file:\n",
    "        json_data = json.load(json_file)\n",
    "\n",
    "    # Load gene annotations from CSV\n",
    "    annotations_df = pd.read_csv(csv_file_path)\n",
    "\n",
    "    # Convert non-string values in 'annotation_text' column to strings\n",
    "    annotations_df['annotation_text'] = annotations_df['annotation_text'].astype(str)\n",
    "\n",
    "    # Extract unique words from the JSON data\n",
    "    all_words = set()\n",
    "    for entry in json_data:\n",
    "        text = entry.get(\"ARTICLE\", {}).get(\"TEXT\", \"\")\n",
    "        words = nltk.word_tokenize(text)\n",
    "        all_words.update(words)\n",
    "\n",
    "    # Tokenize each entry in the 'annotation_text' column\n",
    "    annotations_df['annotation_words'] = annotations_df['annotation_text'].apply(nltk.word_tokenize)\n",
    "\n",
    "    # Filter annotations based on unique words\n",
    "    filtered_annotations = annotations_df[annotations_df['annotation_words'].apply(lambda x: any(word in all_words for word in x))]\n",
    "\n",
    "    # Load pre-trained BioBERT model and tokenizer\n",
    "    config = BertConfig.from_pretrained('dmis-lab/biobert-base-cased-v1.1', output_hidden_states=True)\n",
    "    tokenizer = BertTokenizer.from_pretrained('dmis-lab/biobert-base-cased-v1.1')\n",
    "    model = BertModel.from_pretrained('dmis-lab/biobert-base-cased-v1.1', config=config)\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    # Create different output files for different types of embeddings\n",
    "    output_file_sum = os.path.join(output_directory, f\"BioBERT_Gene_Sum{os.path.basename(csv_file_path).replace('gene_annotations', '').replace('.csv', '.txt')}\")\n",
    "    output_file_last4 = [os.path.join(output_directory, f\"BioBERT_Gene_Layer_{i}{os.path.basename(csv_file_path).replace('gene_annotations', '').replace('.csv', '.txt')}\") for i in range(-4, 0)]\n",
    "\n",
    "    with open(output_file_sum, 'w') as text_file_sum:\n",
    "        text_files_last4 = [open(file_name, 'w') for file_name in output_file_last4]\n",
    "\n",
    "        for phrase in tqdm(set(filtered_annotations['annotation_text']), desc=\"Processing phrases\"):\n",
    "            # Tokenize and encode the phrase\n",
    "            inputs = tokenizer(phrase, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "\n",
    "            # Get the hidden states for the last 4 layers\n",
    "            hidden_states = outputs.hidden_states[-4:]\n",
    "\n",
    "            # Sum of the last 4 layers\n",
    "            sum_embedding = torch.sum(torch.stack(hidden_states), dim=0).squeeze(0)\n",
    "            sum_embedding = torch.sum(sum_embedding, dim=0).tolist()\n",
    "            embedding_str_sum = ', '.join(map(str, sum_embedding))\n",
    "            text_file_sum.write(f\"Phrase:{phrase}, Embedding: [{embedding_str_sum}]\\n\")\n",
    "\n",
    "            # Save each of the last 4 layers individually\n",
    "            for i, hidden_state in enumerate(hidden_states):\n",
    "                layer_embedding = torch.mean(hidden_state.squeeze(0), dim=0).tolist()\n",
    "                embedding_str_layer = ', '.join(map(str, layer_embedding))\n",
    "                text_files_last4[i].write(f\"Phrase:{phrase}, Embedding: [{embedding_str_layer}]\\n\")\n",
    "\n",
    "        # Close individual layer files\n",
    "        for text_file in text_files_last4:\n",
    "            text_file.close()\n",
    "\n",
    "    print(f\"Summation embeddings saved in {output_file_sum}\")\n",
    "    print(f\"Individual layer embeddings saved in {output_directory}\")\n",
    "\n",
    "# Define the directory containing the JSON and CSV files\n",
    "json_dir = \"split_pubtator\"\n",
    "csv_dir = \"csvfiles\"\n",
    "output_directory = \"3gen_wordembeddings/BioBERT/Gene\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Process each JSON file and its corresponding gene annotations CSV file, processing only those ending with \".json\"\n",
    "for file_name in tqdm(os.listdir(json_dir), desc=\"Processing files\"):\n",
    "    if file_name.endswith(\".json\"):\n",
    "        json_file_path = os.path.join(json_dir, file_name)\n",
    "        csv_file_name = f\"gene_annotations_{file_name.replace('proper_pubtator_', '').replace('.json', '')}.csv\"\n",
    "        csv_file_path = os.path.join(csv_dir, csv_file_name)\n",
    "        if os.path.exists(csv_file_path):\n",
    "            process_files(json_file_path, csv_file_path, output_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94c4ef4-0b3f-4131-a7b7-bbe5b9ddfa33",
   "metadata": {},
   "source": [
    "Note: This creates word embeddings from sum of last 4 layers and last 4 layers individually. Naming format will be Sum , -1,-2,-3,-4. indicating last 4 layers. Once the embeddings are created you can sort them into right folders combine the files together in idividual folders to form combined word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e6fde1-d80e-4cef-8fa7-c220f10a7325",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
